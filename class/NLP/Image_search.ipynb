{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a687c92e",
   "metadata": {},
   "source": [
    "It is highly recommended to use a powerful **GPU**, you can use it for free uploading this notebook to [Google Colab](https://colab.research.google.com/notebooks/intro.ipynb).\n",
    "<table align=\"center\">\n",
    " <td align=\"center\"><a target=\"_blank\" href=\"https://colab.research.google.com/github/ezponda/intro_deep_learning/blob/main/class/NLP/Image_search.ipynb\">\n",
    "        <img src=\"https://colab.research.google.com/img/colab_favicon_256px.png\"  width=\"50\" height=\"50\" style=\"padding-bottom:5px;\" />Run in Google Colab</a></td>\n",
    "  <td align=\"center\"><a target=\"_blank\" href=\"https://github.com/ezponda/intro_deep_learning/blob/main/class/NLP/Image_search.ipynb\">\n",
    "        <img src=\"https://github.githubassets.com/images/modules/logos_page/GitHub-Mark.png\"  width=\"50\" height=\"50\" style=\"padding-bottom:5px;\" />View Source on GitHub</a></td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8c43ec",
   "metadata": {},
   "source": [
    "# Image search\n",
    "\n",
    "In this notebook, we'll introduce image search using Sentence Transformers, by mapping images and texts into the same vector space. This enables us to perform search and retrieval tasks for images based on textual descriptions.\n",
    "\n",
    "To achieve this, we'll utilize the [CLIP (Contrastive Language-Image Pretraining)](https://openai.com/research/clip) model, which is designed to learn a joint embedding space for both images and texts.\n",
    "\n",
    "Contrastive Language-Image Pretraining (CLIP) is an AI model developed by OpenAI. It is designed to learn from a wide range of tasks by leveraging the connection between natural language and images.\n",
    "\n",
    "1. Multimodal Learning: CLIP is a multimodal model that can understand both images and text. It is pretrained on a large dataset containing pairs of images and their associated text captions, learning to associate visual concepts with natural language.\n",
    "\n",
    "2. Contrastive Learning: CLIP learns by optimizing a contrastive objective. It is trained to recognize which image-caption pairs are correct among a set of negative examples. By learning to score the correct image-text pairs higher than incorrect ones, the model learns a useful representation for both modalities.\n",
    "\n",
    "3. Architecture: CLIP uses a Transformer-based architecture for processing text and a Vision Transformer or ResNet architecture for processing images. The image and text encoders are jointly trained, allowing the model to align both modalities in a shared embedding space.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e44036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the sentence-transformers library\n",
    "!pip install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985e3a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentence_transformers\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import glob\n",
    "import pickle\n",
    "import zipfile\n",
    "import copy\n",
    "from IPython.display import display\n",
    "from IPython.display import Image as IPImage\n",
    "import os\n",
    "from tqdm.autonotebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa59f447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we load the respective CLIP model\n",
    "model_name = 'clip-ViT-B-32'\n",
    "model = SentenceTransformer(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b22ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from io import StringIO, BytesIO\n",
    "\n",
    "def get_image_from_url(url):\n",
    "    response = requests.get(url)\n",
    "    img = Image.open(BytesIO(response.content))\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300f0287",
   "metadata": {},
   "source": [
    "For searching images, we need an image set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73382b00",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "img_url_path = 'https://github.com/ezponda/intro_deep_learning/raw/main/images/'\n",
    "img_urls = [\n",
    "    f'{img_url_path}eiffel_tower.jpeg',\n",
    "    f'{img_url_path}taj_mahal.jpeg',\n",
    "    f'{img_url_path}colosseum.jpeg',\n",
    "    f'{img_url_path}great_wall_of_china.jpeg',\n",
    "    f'{img_url_path}statue_of_liberty.jpeg',\n",
    "]\n",
    "images = [get_image_from_url(url) for url in img_urls]\n",
    "\n",
    "print('Sample images: ')\n",
    "for url, image in zip(img_urls, images):\n",
    "    print('_'*50)\n",
    "    print(f'url: {url}')\n",
    "    display(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9b644a",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_embeddings = model.encode(images,\n",
    "                       batch_size=128,\n",
    "                       convert_to_tensor=True,\n",
    "                       show_progress_bar=True)\n",
    "img_embeddings = img_embeddings.cpu()\n",
    "print(img_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb617fb8",
   "metadata": {},
   "source": [
    "Now, let's define a function to perform image search, given a query and a list of image embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18e17ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_search(query, model, img_embeddings, images, top_k=2):\n",
    "    query_embedding = model.encode([query])[0]\n",
    "    similarities = cosine_similarity([query_embedding], img_embeddings)[0]\n",
    "    indexes = np.argpartition(similarities, -top_k)[-top_k:]\n",
    "    indexes = indexes[np.argsort(-similarities[indexes])]\n",
    "    print(f\"Input query: {query}\")\n",
    "    print()\n",
    "    for ind, sim in zip(list(indexes), similarities[indexes].tolist()):\n",
    "        print('_'*50)\n",
    "        print(sim)\n",
    "        display(images[ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf9c113",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_search('A building in Paris', model, img_embeddings, images, top_k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7da09ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_search('Find me an image of a famous monument in India', model, img_embeddings, images, top_k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07a8391",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_search('A building in China', model, img_embeddings, images, top_k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e99599",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8fdb5270",
   "metadata": {},
   "source": [
    "## Unsplash subset dataset\n",
    "\n",
    "[Unsplash](https://unsplash.com/data) is a collaborative image dataset openly shared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6420491b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we get about 25k images from Unsplash \n",
    "img_folder = './photos/'\n",
    "if not os.path.exists(img_folder) or len(os.listdir(img_folder)) == 0:\n",
    "    os.makedirs(img_folder, exist_ok=True)\n",
    "    \n",
    "    photo_filename = 'unsplash-25k-photos.zip'\n",
    "    if not os.path.exists(photo_filename):   #Download dataset if does not exist\n",
    "        util.http_get('http://sbert.net/datasets/'+photo_filename, photo_filename)\n",
    "        \n",
    "    #Extract all images\n",
    "    with zipfile.ZipFile(photo_filename, 'r') as zf:\n",
    "        for member in tqdm(zf.infolist(), desc='Extracting'):\n",
    "            zf.extract(member, img_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58298e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we need to compute the embeddings\n",
    "# To speed things up, we destribute pre-computed embeddings\n",
    "# Otherwise you can also encode the images yourself.\n",
    "# To encode an image, you can use the following code:\n",
    "# from PIL import Image\n",
    "# img_emb = model.encode(Image.open(filepath))\n",
    "def read_image_from_path(file_path):\n",
    "    img = Image.open(file_path)\n",
    "    return img\n",
    "\n",
    "use_precomputed_embeddings = True\n",
    "\n",
    "if use_precomputed_embeddings: \n",
    "    emb_filename = 'unsplash-25k-photos-embeddings.pkl'\n",
    "    if not os.path.exists(emb_filename):   #Download dataset if does not exist\n",
    "        util.http_get('http://sbert.net/datasets/'+emb_filename, emb_filename)\n",
    "        \n",
    "    with open(emb_filename, 'rb') as fIn:\n",
    "        img_names, img_embeddings = pickle.load(fIn)  \n",
    "    \n",
    "\n",
    "    print(\"Images:\", len(img_names))\n",
    "else:\n",
    "    img_names = list(glob.glob('photos/*.jpg'))[:5_000]\n",
    "    print(\"Images:\", len(img_names))\n",
    "    images = [read_image_from_path(img_name) for img_name in  img_names]\n",
    "    img_embeddings = model.encode(images, batch_size=128, convert_to_tensor=True, show_progress_bar=True)\n",
    "    img_embeddings = img_embeddings.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce3a523",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_search_from_path(query, model, img_embeddings, img_folder, img_names, top_k=2):\n",
    "    query_embedding = model.encode([query])[0]\n",
    "    similarities = cosine_similarity([query_embedding], img_embeddings)[0]\n",
    "    indexes = np.argpartition(similarities, -top_k)[-top_k:]\n",
    "    indexes = indexes[np.argsort(-similarities[indexes])]\n",
    "    print(f\"Input query: {query}\")\n",
    "    print()\n",
    "    for ind, sim in zip(list(indexes), similarities[indexes].tolist()):\n",
    "        print('_'*50)\n",
    "        print(sim)\n",
    "        path = os.path.join(img_folder, img_names[ind])\n",
    "        img = copy.deepcopy(Image.open(open(path, 'rb')))\n",
    "        display(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4efc717",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_search_from_path('A building in Paris', model, img_embeddings, img_folder, img_names, top_k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833f2fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_search_from_path('A building in China', model, img_embeddings, img_folder, img_names, top_k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4c1443",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_search_from_path('A building in China', model, img_embeddings, img_folder, img_names, top_k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35453318",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_search_from_path('Two dogs playing in the snow', model, img_embeddings, img_folder, img_names, top_k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fee3925",
   "metadata": {},
   "source": [
    "## Image-to-Image Search\n",
    "You can use the method also for image-to-image search.\n",
    "\n",
    "To achieve this, you pass `get_image_from_url(url)` to the search method.\n",
    "\n",
    "It will then return similar images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3b873c",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = get_image_from_url(img_urls[0])\n",
    "image_search_from_path(img, model, img_embeddings, img_folder, img_names, top_k=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}